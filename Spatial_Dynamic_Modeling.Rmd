---
title: "COVID-19 Spatial Dynamics in the US: Spatiotemporal Analysis of Predicting Covid Cases on Correlation Length and Local Correlation with Time-invariant Controls"
subtitle: "Aarushi Somani, Eleanor Kim, Lauren Murai, Meichen Chen"
date: '2024-04-16'
output:
  pdf_document: default
  word_document: default
---

```{r, warning = FALSE}
#load packages
library(sandwich)
library(lmtest)
library(dplyr)
library(tibble)
library(stargazer)
library(car)
```

# Read in 9 Datasets
## All county data + Urban county data with Wave dummies

```{r}
# Weekly Spatial Metrics with Time Wave Columns
all_weekly_spatial_metrics_waves = read.csv("weekly_spatial_metrics_waves.csv")

# same data set but calculations from urban counties only (pop >250k)
urban_weekly_spatial_metrics_waves = read.csv("urban_weekly_spatial_metrics_waves.csv")
```

## All county data + Urban county data with Region + Wave dummies
```{r}
# Weekly Spatial Metrics with Region Columns
all_regional_weekly_spatial_metrics = read.csv("regional_weekly_spatial_metrics.csv")

# same data set but calculations from urban counties only (pop >250k)
urban_regional_weekly_spatial_metrics = read.csv("urban_regional_weekly_spatial_metrics.csv")
```

## All county data + Urban county data with MHHINC + Wave dummies

```{r}
# Weekly Spatial Metrics with Median Household Income Columns
all_mhhinc_weekly_spatial_metrics = read.csv("mhhinc_weekly_spatial_metrics.csv")

# same data set but calculations from urban counties only (pop >250k)
urban_mhhinc_weekly_spatial_metrics = read.csv("urban_mhhinc_weekly_spatial_metrics.csv")
```

## All county data + Urban county data with Region + MHHINC + Wave dummies

```{r}
# Local Corelation with Region + MHHINC Columns
all_region_mhhinc_weekly_localcor = read.csv("region_mhhinc_weekly_localcor.csv")

# same data set but calculations from urban counties only (pop >250k)
urban_region_mhhinc_weekly_localcor = read.csv("urban_region_mhhinc_weekly_localcor.csv")
```


## All county data with Mask Usage + Wave dummies

```{r}
# Weekly Spatial Metrics with Mask Usaege Columns
all_mask_weekly_spatial_metrics = read.csv("all_mask_weekly_spatial_metrics.csv")
```

# Simple Models


## Prepare Model Combinations

```{r}
# Data sets: (1) all counties & (2) urban counties
datasets = list(all_weekly_spatial_metrics_waves, urban_weekly_spatial_metrics_waves)
df_names = c("all","urban")

# Predictor: (1) correlation length & (2) log(local correlation)
predictor = c("cor_lengths", "r_0_50")
pred_names = c("corlength","localcor")
```

## Generate Models

### Training/Testing Models

```{r}
# Set seed for reproducibility
set.seed(1)

# Create an empty list to store the models
simple_lm_models_train <- list()

# Define the train-test split ratio
train_ratio <- 0.8
test_ratio <- 1 - train_ratio

# Apply the trained model to the test data and make predictions
for (d in 1:length(datasets)) {
  df <- datasets[d][[1]]
  n_obs <- nrow(df)
  
  # Create indices for train-test split
  train_indices <- sample(1:n_obs, size = round(train_ratio * n_obs), replace = FALSE)
  test_indices <- setdiff(1:n_obs, train_indices)
  
  # Split the data into train and test sets
  train_df <- df[train_indices, ]
  test_df <- df[test_indices, ]
  
  for (p in 1:length(predictor)) {
     
    # Create the formula
    lm_formula <- as.formula(paste("log(next_week_marginal_cases) ~", predictor[p]))
    
    # Fit the linear model on the train set
    lm_model_train <- lm(lm_formula, data = train_df)
    
    # Generate a name for the model object
    model_name_train <- paste(df_names[d], "_", pred_names[p], "_train", sep = "")
    
    # Save the train model to the list
    simple_lm_models_train[[model_name_train]] <- lm_model_train
    
    # Apply the trained model to the test data and make predictions
    test_predictions <- predict(lm_model_train, newdata = test_df)
    
    # Calculate residuals using predictions from the test dataset
    residuals <- log(test_df$next_week_marginal_cases)-test_predictions
    
    # Create residual plot
    plot(1:length(residuals),residuals,
         main = paste("Residual Plot for", model_name_train),
         xlab = "Actual Values",
         ylab = "Residuals")
    abline(h = 0, col = "blue") 
    
    #is the mean of residuals less than standard deviation of data?
    print(mean(abs(residuals), na.rm=TRUE) < sd(log(test_df$next_week_marginal_cases), na.rm=TRUE))


  }
}

names(simple_lm_models_train)
```

## View Model Summaries

```{r}
for (m in 1:length(simple_lm_models_train)){
  print(summary(simple_lm_models_train[[m]]))
}
```


```{r, warning = FALSE}
# stargazer table for all_corlengths_region and urban_corlengths_region
# (1) is all
# (2) is urban
stargazer(
  c(simple_lm_models_train[1], simple_lm_models_train[3]),
  type = 'text', 
  title = "Regression of Covid Cases on Correlation Length"
)
```

```{r, warning = FALSE}
# all_localcor_wave_region and urban_localcor_wave_region
# (1) is all
# (2) is urban
stargazer(
  c(simple_lm_models_train[2], simple_lm_models_train[4]),
  type = 'text', 
  title = "Regression of Covid Cases on Local Correlation"
)
```

## Check conditions of Normality of Residuals, Homoskedasticity, Linearity, Independence of Residuals

```{r}
for (i in 1:length(simple_lm_models_train)) {
lm_model = simple_lm_models_train[[i]]

# 1. Linearity Check (Visual Inspection)
plot(residuals(lm_model) ~ fitted(lm_model))

# 2. Independence Check# Durbin-Watson test
durbinWatsonTest(lm_model)

# 3. Homoscedasticity Check (Visual Inspection)
# Breusch-Pagan test for Heteroscedasticity
bptest(lm_model)

# 4. Normality of Residuals Check
# Histogram of residuals
hist(residuals(lm_model), main = "Histogram of Residuals")
}

# Heteroskedasticity/Non-linearity in Urban Local Correlation Model
```

## Interpretations

```{r}
units = c(round(sd(all_weekly_spatial_metrics_waves$cor_lengths, na.rm = TRUE)), round( sd(all_weekly_spatial_metrics_waves$r_0_50) ,2))
var = c("correlation length","local correlation")
for (i in 1:4) {
  adj_i = 2-i%%2
  print(paste("a 1 sd increase in ", var[adj_i]," (",units[adj_i], ") corresponds to a ", round(simple_lm_models_train[[i]]$coefficients[2]*units[adj_i]*100,2),"% change in cases in the following week across ", df_names[(i>2)+1]," counties", sep=""))}
```
Effects are less strong when comparing across only the urban counties. Local correlation and correlation length seem to both hold a strong effect in indicating/predicting a rise in cases in the next week.

# Region + Median Household Income Models

## Prepare Model Combinations

```{r}
# Data sets: (1) all counties & (2) urban counties
datasets = list(all_region_mhhinc_weekly_localcor, urban_region_mhhinc_weekly_localcor)
df_names = c("all_","urban_")

# Predictor: (1) correlation length & (2) local correlation
predictor = c("r_0_50")
pred_names = c("localcor")

# Covariates (1) region + mhhinc & (2) region + mhhinc + wave
covariates = list(c("*factor(region)*same_tier","*factor(region)*tier_diff_1","*factor(region)*tier_diff_2"))
var_names = c("region_mhhinc")
```

## Generate Models

```{r}
# Create an empty list to store the models
region_tiers_lm_models <- list()

# Iterate over every combination of data, predictor, and covariates
for (d in 1:length(datasets)) {
  df = datasets[d][[1]]
for (p in 1:length(predictor)) {
  for (c in 1:length(covariates)) {
    interactions = c()
    for (i in 1:length(covariates[[c]])) {
      interactions[i] <- paste(predictor[p], covariates[[c]][i]) }
    
    # Create the formula & fit linear model
    lm_formula <- as.formula(paste("log(next_week_marginal_cases) ~", paste(unlist(interactions), collapse = " + ")))
    lm_model <- lm(lm_formula, data = df)
    
    # Name model and save to list
    model_name <- paste(df_names[d], pred_names[p], "_", var_names[c], sep = "")
    region_tiers_lm_models[[model_name]] <- lm_model}}}

names(region_tiers_lm_models)
```

### Training/Testing Models
```{r}
# Set seed for reproducibility
set.seed(1)

# Create an empty list to store the models
region_mhhinc_lm_models_train <- list()

# Iterate over every combination of data, predictor, and covariates
for (d in 1:length(datasets)) {
  df <- datasets[d][[1]]
  
  # Create indices for train-test split
  n_obs <- nrow(df)
  train_indices <- sample(1:n_obs, size = round(train_ratio * n_obs), replace = FALSE)
  test_indices <- setdiff(1:n_obs, train_indices)
  
  # Split the data into train and test sets
  train_df <- df[train_indices, ]
  test_df <- df[test_indices, ]
  
  for (p in 1:length(predictor)) {
    for (c in 1:length(covariates)) {
      interactions <- c()
      for (i in 1:length(covariates[[c]])) {
        interactions[i] <- paste(predictor[p], covariates[[c]][i])
      }
      
      # Create the formula & fit linear model on the training data
      lm_formula <- as.formula(paste("log(next_week_marginal_cases) ~", paste(unlist(interactions), collapse = " + ")))
      lm_model_train <- lm(lm_formula, data = train_df)
      
      # Name model and save to list
      model_name_train <- paste(df_names[d], pred_names[p], "_", var_names[c], "_train", sep = "")
      region_mhhinc_lm_models_train[[model_name_train]] <- lm_model_train
      
      # Apply the trained model to the test data and make predictions
      test_predictions <- predict(lm_model_train, newdata = test_df)
      
      # Calculate residuals using predictions from the test dataset
      residuals <- log(test_df$next_week_marginal_cases) - test_predictions
      
      # Create residual plot
      plot(1:length(residuals), residuals,
           main = paste("Residual Plot for", model_name_train),
           xlab = "Actual Values",
           ylab = "Residuals")
      abline(h = 0, col = "blue")
      
      # Check if the mean of residuals is less than the standard deviation of the data
      print(mean(abs(residuals), na.rm = TRUE) < sd(log(test_df$next_week_marginal_cases), na.rm = TRUE))
    }
  }
}

names(region_mhhinc_lm_models_train)
```
Urban fit is suspicious

## View Model Summaries

```{r}
for (m in 1:length(region_mhhinc_lm_models_train)){
  print(summary(region_mhhinc_lm_models_train[[m]]))
}
```

```{r}
# Correlation Length Models
stargazer(
  region_mhhinc_lm_models_train,
  type = 'text'
)
```

## Check conditions of Normality of Residuals, Homoskedasticity,Linearity , No Perfect Multicollinearity, Independence of Residuals

```{r}
for (i in 1:length(region_mhhinc_lm_models_train)) {
lm_model = region_mhhinc_lm_models_train[[i]]

# 1. Linearity Check (Visual Inspection)
plot(residuals(lm_model) ~ fitted(lm_model))

# 2. Independence Check
# Durbin-Watson test
print(durbinWatsonTest(lm_model)[3])


# 3. Homoscedasticity Check
# Breusch-Pagan test for Heteroscedasticity
print(bptest(lm_model)$p.value)

# 4. Normality of Residuals Check (Visual Inspection)
hist(residuals(lm_model), main = "Histogram of Residuals")
}
# 5. No Perfect Multicollinearity Check
# Variance Inflation Factors (VIF)
vif(lm(log(next_week_marginal_cases) ~  r_0_50 + factor(region) + factor(mhhinc_tiers) , data = all_region_mhhinc_weekly_localcor))

# Heteroskedasticity in the urban model --> use robust standard errors
```



## Functions to print significant results

```{r}
# Create function that prints interpretation of coefficients

print_interpretation <- function(coef, wave_val = "every wave",region_val = "whole US", tier_descr_val = "are disregarded", significant=TRUE, pred = "r_0_50", county="all counties", df=weekly_spatial_metrics_waves, usage_descr_val= "are disregarded") {
  if (!is.na(coef) && isTRUE(significant)) {
    
    # Prep the data frame to calculate appropriate sd
    if (wave_val !="every wave") {
    df = filter(df, wave == as.numeric(gsub("\\D", "", wave_val))) }
    if (region_val !="whole US") {
    df = filter(df, region == region_val)}
    if (tier_descr_val != "are disregarded") {
    df = filter(df, tier_descr == tier_descr_val)}

    # Calculate sd units
    if (pred == "cor_lengths") {
      # factor_value = round(sd(df$cor_lengths, na.rm = TRUE),2)
      # constant sd units
      factor_value = round(sd(all_weekly_spatial_metrics_waves$cor_lengths, na.rm = TRUE))
      pred_type = paste("correlation length (",factor_value ," km)",sep="")
    } else {
      # factor_value = round(sd(df$r_0_50, na.rm = TRUE),2)
      factor_value = round(sd(all_weekly_spatial_metrics_waves$r_0_50, na.rm = TRUE), 2)
      pred_type = paste("local correlation (",factor_value,")",sep="")
    }
    
    # To Print
    cat("- For", county,"across", wave_val, "in the", region_val, "where the median household income tiers", tier_descr_val, "\n")
    cat(" and where mask usage in both counties", usage_descr_val, "a 1 sd increase", pred_type,"\n")
    cat(" corresponds to a ", round(coef * factor_value * 100, 2), "% increase in cases in the following week. Significant? ", significant, "\n", sep = "")
  }
}

# Create function that returns coefficient
save_coefficient <- function(interaction) {
  if (!is.na(interaction)) {
    coef <- interaction + coefficients[2]
    return(coef)
  } else {
    return(NA)
  }
}

# Create function to calculate significance of coefficient based on t-statistic
is_significant <- function(coef, se, threshold = 2.57) { 
  if (!is.na(coef) && !is.na(se)) {
    # Calculate t-statistic
    t_stat <- coef / se
    # Check if absolute value of t-statistic exceeds threshold for significance
    return(t_stat > threshold) # for 99% confidence 
  } else {
    return(FALSE)  # Treat missing values as not significant
  }
}
```

## Interpretations

```{r}
# Iterate over Regions and Tier diff dummies
for (x in 1:length(region_mhhinc_lm_models_train)) {
  lm_model <- region_mhhinc_lm_models_train[[x]]
  
  if (x == 1) {
    county <- "all counties"
    df = all_region_mhhinc_weekly_localcor
  } else {
    county <- "urban counties"
    df = urban_region_mhhinc_weekly_localcor
  }
  
  # Get stats
  coefficients <- coef(lm_model)
  coef_names <- names(coef(lm_model))
  se <- sqrt(diag(vcovHC(lm_model)))
  se_named <- rep(NA, length(coef_names))
  se_named[names(se)] <- se
  st_errors <- se_named[coef_names]
  
  # Initialize values
  regions <- c("Northeast", "South", "West", "Midwest")
  tier_diff <- c("same_tier", "tier_diff_1", "tier_diff_2", "tier_diff_3")
  tier_descr <- c("are the same", "differ by 1", "differ by 2", "differ by 3")
  full_coef <- c()
  significant <- c()
  i <- 1
  
  # Iterate over Regions and Tier diff dummies
  for (r in 1:length(regions)) {
    region <- regions[r]
    for (m in 1:length(tier_diff)) {
      tierdiff <- tier_diff[m]
      if (region == "Midwest") {
        partial_interaction_r <- 0
        se_partial_r <- 0
      } else {
        partial_interaction_r <- coefficients[(8 + r)]
        se_partial_r <- st_errors[(8 + r)]
      }
      if (tierdiff == "tier_diff_3") {
        partial_interaction_m <- 0
        se_partial_m <- 0
      } else {
        partial_interaction_m <- coefficients[(4*(m+2))]
        se_partial_m <- st_errors[(4*(m+2))]
      } 
      if (tierdiff != "tier_diff_3" & region != "Midwest") {
        full_interaction_rm <- coefficients[paste0("r_0_50:factor(region)", region, ":", tierdiff)]
        se_full_rm <- st_errors[paste0("r_0_50:factor(region)", region, ":", tierdiff)]
      } else {
        full_interaction_rm <- 0
        se_full_rm <- 0
      }
      
      # Calculate total standard error
      se_total <- sqrt(se_partial_r^2 + se_partial_m^2 + se_full_rm^2 + st_errors[2]^2)
      
      # Store coefficient information
      interaction <- partial_interaction_r + partial_interaction_m + full_interaction_rm
      full_coef[i] <- save_coefficient(interaction) 
      
      # Print interpretation and significance
      significant[i] <- is_significant(as.numeric(full_coef[i]), as.numeric((se_total)))
      print_interpretation(full_coef[i], wave= "every wave",region, tier_descr[m], significant[i], pred = "local_cor", county, df)
      i <- i + 1
    }
  }
}
```

Regional and equality level effects are significant in the South and Midwest across every wave. Slightly stronger effects in the South than the midwest among counties of similar median household income tier. Effects are not as discernible among Urban counties. We don't have enough data to calculate correlation length but local correlation is an effective metric here.

# Wave Models

## Prepare Model Combinations
```{r}
# Data sets: (1) all counties & (2) urban counties
datasets = list(all_weekly_spatial_metrics_waves, urban_weekly_spatial_metrics_waves)
df_names = c("all","urban")

# Predictor: (1) correlation length & (2) log(local correlation)
predictor = c("cor_lengths", "r_0_50")
pred_names = c("corlength","localcor")
```

## Generate Models

```{r}
wave_lm_models <- list()

for (d in 1:length(datasets)) {
  df = datasets[[d]]
  for (p in 1:length(predictor)) {
    # Create the formula by including wave_1 and wave_2 as covariates
    lm_formula <- as.formula(paste("log(next_week_marginal_cases) ~", paste(predictor[p],"*factor(wave)")))
    
    # Fit the linear model
    lm_model <- lm(lm_formula, data = df)
    
    # Generate a name for the model object
    model_name <- paste(df_names[d], "_", pred_names[p], "_waves", sep = "")
    
    # Save the model to the list
    wave_lm_models[[model_name]] <- lm_model
  }
}
names(wave_lm_models)
```

### Training/Testing Models

```{r}
# Set seed for reproducibility
set.seed(1)

# Create an empty list to store the models
wave_lm_models_train <- list()

# Iterate over every combination of data, predictor, and covariates
for (d in 1:length(datasets)) {
  df <- datasets[[d]]
  
  # Create indices for train-test split
  n_obs <- nrow(df)
  train_indices <- sample(1:n_obs, size = round(train_ratio * n_obs), replace = FALSE)
  test_indices <- setdiff(1:n_obs, train_indices)
  
  # Split the data into train and test sets
  train_df <- df[train_indices, ]
  test_df <- df[test_indices, ]
  
  for (p in 1:length(predictor)) {
    # Create the formula
    lm_formula <- as.formula(paste("log(next_week_marginal_cases) ~", paste(predictor[p], "*factor(wave)")))
    
    # Fit linear model on the training data
    lm_model_train <- lm(lm_formula, data = train_df)
    
    # Generate a name for the model object
    model_name_train <- paste(df_names[d], "_", pred_names[p], "_waves_train", sep = "")
    
    # Save the train model to the list
    wave_lm_models_train[[model_name_train]] <- lm_model_train
    
    # Apply the trained model to the test data and make predictions
    test_predictions <- predict(lm_model_train, newdata = test_df)
    
    # Calculate residuals using predictions from the test dataset
    residuals <- log(test_df$next_week_marginal_cases) - test_predictions
    
    # Create residual plot
    plot(1:length(residuals), residuals,
         main = paste("Residual Plot for", model_name_train),
         xlab = "Actual Values",
         ylab = "Residuals")
    abline(h = 0, col = "blue")
    
    # Check if the mean of residuals is less than the standard deviation of the data
    print(mean(abs(residuals), na.rm = TRUE) < sd(log(test_df$next_week_marginal_cases), na.rm = TRUE))
  }
}

# Print the names of the models
names(wave_lm_models_train)
```

## View Model Summaries
```{r}
for (m in 1:length(wave_lm_models_train)) {
  print(summary(wave_lm_models_train[[m]]))
}
```

```{r, warning = FALSE}
# stargazer table for all_corlengths_region and urban_corlengths_region
# (1) is all
# (2) is urban
stargazer(
  c(wave_lm_models_train[1], wave_lm_models_train[3]),
  type = "text", 
  title = "Regression of Covid Cases on Correlation Length with Waves"
)
```

```{r, warning = FALSE}
# all_localcor_wave_region and urban_localcor_wave_region
# (1) is all
# (2) is urban
stargazer(
  c(wave_lm_models_train[2], wave_lm_models_train[4]),
  type = 'text', 
  title = "Regression of Covid Cases on Local Correlation with Waves"
)
```

## Check conditions of Normality of Residuals, Homoskedasticity,Linearity , No Perfect Multicollinearity, Independence of Residuals

```{r}
for (i in 1:length(wave_lm_models_train)) {
  lm_model = wave_lm_models_train[[i]]
  
  # Linearity Check (Visual Inspection)
  plot(residuals(lm_model) ~ fitted(lm_model))
  
  # 2. Independence Check
  # Durbin-Watson test
  durbinWatsonTest(lm_model)
  
  # 3. Homoscedasticity Check (Visual Inspection)
  # Breusch-Pagan test for Heteroscedasticity
  bptest(lm_model)
  
  # 4. Normality of Residuals Check
  # Histogram of residuals
  hist(residuals(lm_model), main = "Histogram of Residuals")
}

# Non-linearity  in all_corlength_waves and urban_localcor_waves
```

## Interpretations
### Print Significant Results

```{r}
# Lauren's 

print_significant_wave_interpretations <- function(models) {
  for (i in 1:length(models)) {
    model <- models[[i]]
    coefficients <- summary(model)$coefficients
    
    # Print significant wave coefficients at 0.05 significance level
    for (wave in 2:3) {
      coef_name <- paste("factor(wave)", wave, sep="")
      if (coef_name %in% rownames(coefficients)) {
        coef_val <- coefficients[coef_name, "Estimate"]
        p_value <- coefficients[coef_name, "Pr(>|t|)"]
        
        if (p_value < 0.05) {  
          percent_change <- round(coef_val * 100, 2)
          
          cat("- Among all", ifelse(i <= 2, "all", "urban"), "counties in wave", wave, "\n")
          cat("a 1 unit increase in wave ", wave, " corresponds to a ", 
              percent_change, "% change in cases in the following week. Significant? TRUE\n\n", sep = "")
        }
      }
    }
  }
}
print_significant_wave_interpretations(wave_lm_models)
```

```{r}
# Initialize values
predictor <- c("cor_lengths","r_0_50")
waves <- c("wave 1","wave 2", "wave 3")
full_coef = c()
significant = c()
i=1

# Iterate over Regions and Wave dummies
for (model in 1:length(wave_lm_models_train)) {
  
  # Get model stats
  lm_model = wave_lm_models_train[[model]]
  coefficients <- coef(lm_model)
  coef_names <- names(coef(lm_model))
  
  # Get stats
  # se <- summary(lm_model)$coefficients[, "Std. Error"] use robust SE's
  se <- sqrt(diag(vcovHC(lm_model)))
  se_named <- rep(NA, length(coef_names))
  se_named[names(se)] <- se
  st_errors <- se_named[coef_names]
  
  # Based on model, decide if urban or all county df
  if (model <=2) {
    county = "all counties"
    df = all_weekly_spatial_metrics_waves}
 else {
    county = "urban counties"
    df = urban_weekly_spatial_metrics_waves}
  
  # Based on model, choose cor_length or local_cor as predictor
  if (model %in% c(1,3)) {pred = predictor[1]}
  else {pred = predictor[2]}
    
  # Iterate over each wave model
  for (w in 1:length(waves)) {
    wave <- waves[w]
    if (wave != "wave 1") {
      full_interaction_w <- coefficients[paste0(pred,":factor(wave)", w)]
      se_full_w <- st_errors[paste0(pred,":factor(wave)", w)]
    } else {
      full_interaction_w <- 0
      se_full_w <- 0}
    # Calculate total standard error
    se_total <- sqrt(se_full_w^2+ st_errors[2]^2)
    # Store coefficient information
    interaction <- full_interaction_w
    
    # Store coefficient
    full_coef[i] <- save_coefficient(interaction)
    
    # Print interpretation and significance
    significant[i] <- is_significant(as.numeric(full_coef[i]), as.numeric(se_total))
    print_interpretation(full_coef[i], wave, "whole US", "are disregarded", significant[i], pred, county, df)
    i <- i + 1
    }}
```

# Region + Wave Models

## Prepare Model Combinations

```{r}
# Data sets: (1) all counties & (2) urban counties
datasets = list(all_regional_weekly_spatial_metrics, urban_regional_weekly_spatial_metrics)
df_names = c("all","urban")

# Predictor: (1) correlation length & (2) local correlation
predictor = c("cor_lengths", "r_0_50") #unlog local cor
predictor_names = c("corlengths","localcor")

# Covariates (1) mhhinc & (2) mhhinc + wave
covariates = list(c("*factor(wave)*northeast","*factor(wave)*midwest","*factor(wave)*south"),
                  c("*northeast","*midwest","*south"))
var_names = c("wave_region","region")
```

## Generate Models

```{r, warning = FALSE}
# Create an empty list to store the models
region_lm_models <- list()

# Iterate of every combination of data, predictor, and covariates
for (d in 1:length(datasets)) {
  df = datasets[d][[1]]
  for (p in 1:length(predictor)) {
    for (c in 1:length(covariates)) {
      interactions = c()
      for (i in 1:length(covariates[[c]])) {
        interactions[i] <- paste(predictor[p], covariates[[c]][i])}
     
      # Create the formula
      lm_formula <- as.formula(paste("log(next_week_marginal_cases) ~", paste(unlist(interactions), collapse = " + ")))
      
      # Fit the linear model
      lm_model <- lm(lm_formula, data = df)
      
      # Generate a name for the model object
      model_name <- paste(df_names[d],"_", predictor_names[p], "_", var_names[c], sep = "")
      
      # Save the model to the list
      region_lm_models[[model_name]] <- lm_model
    }}}
```

```{r}
print(names(region_lm_models))
```

### Training/Testing Models
```{r}
# Set seed for reproducibility
set.seed(1)

# Create an empty list to store the models
region_lm_models_train <- list()

# Iterate over every combination of data, predictor, and covariates
for (d in 1:length(datasets)) {
  df <- datasets[[d]]
  
  # Create indices for train-test split
  n_obs <- nrow(df)
  train_indices <- sample(1:n_obs, size = round(train_ratio * n_obs), replace = FALSE)
  test_indices <- setdiff(1:n_obs, train_indices)
  
  # Split the data into train and test sets
  train_df <- df[train_indices, ]
  test_df <- df[test_indices, ]
  
  for (p in 1:length(predictor)) {
    for (c in 1:length(covariates)) {
      interactions <- c()
      for (i in 1:length(covariates[[c]])) {
        interactions[i] <- paste(predictor[p], covariates[[c]][i])
      }
      
      # Create the formula & fit linear model on the training data
      lm_formula <- as.formula(paste("log(next_week_marginal_cases) ~", paste(unlist(interactions), collapse = " + ")))
      lm_model_train <- lm(lm_formula, data = train_df)
      
      # Name model and save to list
      model_name_train <- paste(df_names[d], "_", predictor_names[p], "_", var_names[c], "_train", sep = "")
      region_lm_models_train[[model_name_train]] <- lm_model_train
      
      # Apply the trained model to the test data and make predictions
      test_predictions <- predict(lm_model_train, newdata = test_df)
      
      # Calculate residuals using predictions from the test dataset
      residuals <- log(test_df$next_week_marginal_cases) - test_predictions
      
      # Create residual plot
      plot(1:length(residuals), residuals,
           main = paste("Residual Plot for", model_name_train),
           xlab = "Actual Values",
           ylab = "Residuals")
      abline(h = 0, col = "blue")
      
      # Check if the mean of residuals is less than the standard deviation of the data
      print(mean(abs(residuals), na.rm = TRUE) < sd(log(test_df$next_week_marginal_cases), na.rm = TRUE))
    }
  }
}

# Print the names of the models
names(region_lm_models_train)

```

## View Model Summaries

```{r}
for (m in 1:length(region_lm_models_train)){
  print(summary(region_lm_models_train[[m]]))
}
```
all_localcor_region and urban_localcor_region do not have
significant interaction terms, so we will not make a stargazer table 
for this model. Will make 3 stargazer tables, for these pairs of models:
  (1) all_corlengths_wave_region and urban_corlengths_wave_region
  (2) all_corlengths_region and urban_corlengths_region
  (3) all_localcor_wave_region and urban_localcor_wave_region
  
```{r, warning = FALSE}
# stargazer table for all_corlengths_wave_region and urban_corlengths_wave_region
# (1) is all
# (2) is urban 
stargazer(
  c(region_lm_models_train[1], region_lm_models_train[5]),
  type = 'text', 
  title = "Regression of Covid Cases on Correlation Length, Wave and Region"
)
```

```{r, warning = FALSE}
# stargazer table for all_corlengths_region and urban_corlengths_region
# (1) is all
# (2) is urban
stargazer(
  c(region_lm_models_train[2], region_lm_models_train[6]),
  type = 'text', 
  title = "Regression of Covid Cases on Correlation Length and Region"
)
```

```{r, warning = FALSE}
# all_localcor_wave_region and urban_localcor_wave_region
# (1) is all
# (2) is urban
stargazer(
  c(region_lm_models_train[3], region_lm_models_train[7]),
  type = 'text', 
  title = "Regression of Covid Cases on Local Correlation, Wave and Region"
)
```

## Check conditions of Normality of Residuals, Homoskedasticity,Linearity , No Perfect Multicollinearity, Independence of Residuals

```{r}
for (lm_model in region_lm_models_train) {
  
# 1. Linearity Check (Visual Inspection)
plot(residuals(lm_model) ~ fitted(lm_model))

# 2. Independence Check
# Durbin-Watson test
print(durbinWatsonTest(lm_model)[3])

# 3. Homoscedasticity Check
# Breusch-Pagan test for Heteroscedasticity
print(bptest(lm_model)$p.value)

# 4. Normality of Residuals Check (Visual Inspection)
hist(residuals(lm_model), main = "Histogram of Residuals")
}

# 5. No Perfect Multicollinearity Check
# Variance Inflation Factors (VIF)
vif(lm(log(next_week_marginal_cases) ~  cor_lengths + factor(wave) + factor(region) , data = all_regional_weekly_spatial_metrics))
vif(lm(log(next_week_marginal_cases) ~  cor_lengths + factor(wave) + factor(region) , data = urban_regional_weekly_spatial_metrics))


names(region_lm_models)

# Conditions not passed:
## Heteroskedastic in all_corlengths_wave_region, all_corlengths_region, urban_corlengths_region, urban_localcor_region
## Non-linearity in urban_localcor_region
## solutions --> use robust standard errors, reject a 99% level for strong results
```

## Interpretations

```{r}

# Initialize values
predictor <- c("cor_lengths","r_0_50")
waves <- c("wave 1","wave 2", "wave 3")
regions <- c("Northeast", "Midwest", "South", "West")
full_coef = c()
full_coef1 = c()
significant = c()
significant1 = c()
i=1
j=1

# Iterate over Regions and Wave dummies
for (model in 1:length(region_lm_models_train)) {
  
  # Get model stats
  lm_model = region_lm_models_train[[model]]
  coefficients <- coef(lm_model)
  coef_names <- names(coef(lm_model))
  
   # Get stats
  # se <- summary(lm_model)$coefficients[, "Std. Error"] use robust SE's
  se <- sqrt(diag(vcovHC(lm_model)))
  se_named <- rep(NA, length(coef_names))
  se_named[names(se)] <- se
  st_errors <- se_named[coef_names]
  
  # Based on model, decide if urban or all county df
  if (model <=4) {
    county = "all counties"
    df = all_regional_weekly_spatial_metrics}
  else {
    county = "urban counties"
    df = urban_regional_weekly_spatial_metrics}
  
  # Based on model, choose cor_length or local_cor as predictor
  if (model %in% c(1,2,5,6)) {pred = predictor[1]}
  else {pred = predictor[2]}
    
  # Wave dummies not included for even models
  if (model %% 2 == 0) {
    wave = "every wave"
    for (r in 1:length(regions)) {
      region <- regions[r]
      if (r==4) {interaction = 0
      se_total = st_errors[2]}
      else {
      interaction = coefficients[5+r]
      se_total = sqrt(sum((st_errors[5+r])^2 + (st_errors[2])^2))
      }
      # Store coefficient
      full_coef1[j] = save_coefficient(interaction)
    
      # Print interpretation and significance
      significant1[j] <- is_significant(as.numeric(full_coef1[j]), as.numeric((se_total)))
      print_interpretation(full_coef1[j], wave, region, "are disregarded", significant1[j], pred, county, df)}}
  
  # Wave dummies with mhhinc tier dummies
  else {
  # Iterate over each wave    
  for (w in 1:length(waves)) {
    wave <- waves[w]
    # Iterate of each tier difference
    for (r in 1:length(regions)) {
      region <- regions[r]
      if (wave == "wave 1") {
        partial_interaction_w <- 0
        se_partial_w <- 0
      } else {
        partial_interaction_w <- coefficients[(6 + w)]
        se_partial_w <- st_errors[(6 + w)]
      }
      if (region == "West") {
        partial_interaction_r <- 0
        se_partial_r <- 0
      } else {
        partial_interaction_r <- coefficients[(3*r+7)]
        se_partial_r <- st_errors[(3*r+7)]
      } 
      if (region != "West" & wave != "wave 1") {
        full_interaction_wr <- coefficients[paste0(pred,":factor(wave)", w, ":", tolower(region))]
        se_full_wr <- st_errors[paste0(pred,":factor(wave)", w, ":", region)]
      } else {
        full_interaction_wr <- 0
        se_full_wr <- 0
      }
    # Calculate total standard error
    se_total <- sqrt(se_partial_w^2 + se_partial_r^2 + se_full_wr^2 + st_errors[2]^2)
    # Store coefficient information
    interaction <- partial_interaction_w + partial_interaction_r + full_interaction_wr
    
    # Store coefficient
    full_coef[i] = save_coefficient(interaction)
    
    # Print interpretation and significance
    significant[i] <- is_significant(as.numeric(full_coef[i]), as.numeric(se_total))
    print_interpretation(full_coef[i], wave, region, "are disregarded", significant[i], pred, county, df)
    i <- i + 1
    }}}
}
```

# MHHINC + Wave Models

## Prepare Model Combinations

```{r}
# Data sets: (1) all counties & (2) urban counties
datasets = list(all_mhhinc_weekly_spatial_metrics, urban_mhhinc_weekly_spatial_metrics)
df_names = c("all","urban")

# Predictor: (1) correlation length & (2) local correlation
predictor = c("cor_lengths", "r_0_50") #unlog local cor
predictor_names = c("corlengths","localcor")

# Covariates (1) mhhinc & (2) mhhinc + wave
covariates = list(c("*factor(wave)*same_tier","*factor(wave)*tier_diff_1","*factor(wave)*tier_diff_2"),
                  c("*same_tier","*tier_diff_1","*tier_diff_2"))
var_names = c("wave_mhhinc","mhhinc")
```

## Generate Models

```{r, warning = FALSE}
# Create an empty list to store the models
mhhinc_lm_models <- list()

# Iterate of every combination of data, predictor, and covariates
for (d in 1:length(datasets)) {
  df = datasets[d][[1]]
  for (p in 1:length(predictor)) {
    for (c in 1:length(covariates)) {
      interactions = c()
      for (i in 1:length(covariates[[c]])) {
        interactions[i] <- paste(predictor[p], covariates[[c]][i])}
     
      # Create the formula
      lm_formula <- as.formula(paste("log(next_week_marginal_cases) ~", paste(unlist(interactions), collapse = " + ")))
      
      # Fit the linear model
      lm_model <- lm(lm_formula, data = df)
      
      # Generate a name for the model object
      model_name <- paste(df_names[d],"_", predictor_names[p], "_", var_names[c], sep = "")
      
      # Save the model to the list
      mhhinc_lm_models[[model_name]] <- lm_model
    }}}
```

```{r}
print(names(mhhinc_lm_models))
```

### Training/Testing Models

```{r}
# Set seed for reproducibility
set.seed(1)

# Create an empty list to store the models
mhhinc_lm_models_train <- list()

# Iterate over every combination of data, predictor, and covariates
for (d in 1:length(datasets)) {
  df <- datasets[[d]]
  
  # Create indices for train-test split
  n_obs <- nrow(df)
  train_indices <- sample(1:n_obs, size = round(train_ratio * n_obs), replace = FALSE)
  test_indices <- setdiff(1:n_obs, train_indices)
  
  # Split the data into train and test sets
  train_df <- df[train_indices, ]
  test_df <- df[test_indices, ]
  
  for (p in 1:length(predictor)) {
    for (c in 1:length(covariates)) {
      interactions <- c()
      for (i in 1:length(covariates[[c]])) {
        interactions[i] <- paste(predictor[p], covariates[[c]][i])
      }
      
      # Create the formula & fit linear model on the training data
      lm_formula <- as.formula(paste("log(next_week_marginal_cases) ~", paste(unlist(interactions), collapse = " + ")))
      lm_model_train <- lm(lm_formula, data = train_df)
      
      # Name model and save to list
      model_name_train <- paste(df_names[d], "_", predictor_names[p], "_", var_names[c], "_train", sep = "")
      mhhinc_lm_models_train[[model_name_train]] <- lm_model_train
      
      # Apply the trained model to the test data and make predictions
      test_predictions <- predict(lm_model_train, newdata = test_df)
      
      # Calculate residuals using predictions from the test dataset
      residuals <- log(test_df$next_week_marginal_cases) - test_predictions
      
      # Create residual plot
      plot(1:length(residuals), residuals,
           main = paste("Residual Plot for", model_name_train),
           xlab = "Actual Values",
           ylab = "Residuals")
      abline(h = 0, col = "blue")
      
      # Check if the mean of residuals is less than the standard deviation of the data
      print(mean(abs(residuals), na.rm = TRUE) < sd(log(test_df$next_week_marginal_cases), na.rm = TRUE))
    }
  }
}

# Print the names of the models
names(mhhinc_lm_models_train)

```

Seems like the models are unreliable for the urban + local correlation models

## View Model Summaries

```{r}
for (m in 1:length(mhhinc_lm_models_train)){
  print(summary(mhhinc_lm_models_train[[m]]))
}
```

all_corlengths_wave_mhhinc and urban_corlengths_wave_mhhinc do not have
significant interaction terms, will not make a stargazer table for this
model. Will make 3 stargazer tables, for these pairs of models:
  (1) all_corlengths_mhhinc and urban_corlengths_mhhinc
  (2) all_localcor_wave_mhhinc and urban_localcor_wave_mhhinc
  (3) all_localcor_mhhinc and urban_localcor_mhhinc

```{r, warning = FALSE}
# stargazer table for all_corlengths_mhhinc and urban_corlengths_mhhinc
# (1) is all
# (2) is urban 
stargazer(
  c(mhhinc_lm_models_train[2], mhhinc_lm_models_train[6]),
  type = 'text', 
  title = "Regression of Covid Cases on Correlation Length and Mhhinc Tier Difference"
)
```

```{r, warning = FALSE}
# stargazer table for all_localcor_wave_mhhinc and urban_localcor_wave_mhhinc
# (1) is all
# (2) is urban
stargazer(
  c(mhhinc_lm_models_train[3], mhhinc_lm_models_train[7]),
  type = 'text', 
  title = "Regression of Covid Cases on Local Correlation,Mhhinc Tier Difference, and Wave"
)
```

```{r, warning = FALSE}
# all_localcor_mhhinc and urban_localcor_mhhinc
# (1) is all
# (2) is urban
stargazer(
  c(mhhinc_lm_models[4], mhhinc_lm_models[8]),
  type = 'text', 
  title = "Regression of Covid Cases on Local Correlation and Mhhinc Tier Difference"
)
```

## Check conditions of Normality of Residuals, Homoskedasticity,Linearity , No Perfect Multicollinearity, Independence of Residuals

```{r}
for (lm_model in mhhinc_lm_models_train) {
# 1. Linearity Check (Visual Inspection)
plot(residuals(lm_model) ~ fitted(lm_model))

# 2. Independence Check
# Durbin-Watson test
print(durbinWatsonTest(lm_model)[3])

# 3. Homoscedasticity Check
# Breusch-Pagan test for Heteroscedasticity
print(bptest(lm_model)$p.value)

# 4. Normality of Residuals Check (Visual Inspection)
hist(residuals(lm_model), main = "Histogram of Residuals")
}

# 5. No Perfect Multicollinearity Check
# Variance Inflation Factors (VIF)
vif(lm(log(next_week_marginal_cases) ~  cor_lengths + factor(wave) + factor(mhhinc_tier) , data = all_mhhinc_weekly_spatial_metrics))
vif(lm(log(next_week_marginal_cases) ~  cor_lengths + factor(wave) + factor(mhhinc_tier) , data = urban_mhhinc_weekly_spatial_metrics))

# Conditions not passed:
## Heteroskedasticity in all_corlengths_wave_mhhinc_train, all_localcor_wave_mhhinc_train, all_localcor_mhhinc, all urban models
## Non-linearity in urban_localcor_wave_mhhinc, urban_localcor_mhhinc
names(mhhinc_lm_models_train)
```

## Interpretations
```{r}

# Initialize values
predictor <- c("cor_lengths","r_0_50")
waves <- c("wave 1","wave 2", "wave 3")
tier_diff <- c("same_tier", "tier_diff_1", "tier_diff_2", "tier_diff_3")
tier_descr <- c("are the same", "differ by 1", "differ by 2", "differ by 3")
full_coef = c()
full_coef1 = c()
significant = c()
significant1 = c()
i=1
j=1

# Iterate over Wave and Tier diff dummies
for (model in 1:length(mhhinc_lm_models_train)) {
  
  # Get model stats
  lm_model = mhhinc_lm_models_train[[model]]
  coefficients <- coef(lm_model)
  coef_names <- names(coef(lm_model))
  # se <- summary(lm_model)$coefficients[, "Std. Error"] use robust SE's
  se <- sqrt(diag(vcovHC(lm_model)))
  se_named <- rep(NA, length(coef_names))
  se_named[names(se)] <- se
  st_errors <- se_named[coef_names]
  
  # Based on model, decide if urban or all county df
  if (model <=4) {
    county = "all counties"
    df = all_mhhinc_weekly_spatial_metrics}
  else {
    county = "urban counties"
    df = urban_mhhinc_weekly_spatial_metrics}
  
  # Based on model, choose cor_length or local_cor as predictor
  if (model %in% c(1,2,5,6)) {pred = predictor[1]}
  else {pred = predictor[2]}
    
  # Wave dummies not included for even models
  if (model %% 2 == 0) {
    wave = "every wave"
    for (m in 1:length(tier_diff)) {
      tierdiff <- tier_diff[m]
      if (m==4) {interaction = 0
      se_total = st_errors[2]}
      else {
      interaction = coefficients[5+m]
      se_total = sqrt(sum((st_errors[5+m])^2 + (st_errors[2])^2))
      }
      # Store coefficient
      full_coef1[j] = save_coefficient(interaction)
    
      # Print interpretation and significance
      significant1[j] <- is_significant(as.numeric(full_coef1[j]), as.numeric((se_total)))
      print_interpretation(full_coef1[j], wave, region="whole US", tier_descr[m], significant1[j], pred, county, df)}}
  
  # Wave dummies with mhhinc tier dummies
  else {
  # Iterate over each wave    
  for (w in 1:length(waves)) {
    wave <- waves[w]
    # Iterate of each tier difference
    for (m in 1:length(tier_diff)) {
      tierdiff <- tier_diff[m]
      if (wave == "Wave 1") {
        partial_interaction_w <- 0
        se_partial_w <- 0
      } else {
        partial_interaction_w <- coefficients[(7 + w)]
        se_partial_w <- st_errors[(7 + w)]
      }
      if (tierdiff == "tier_diff_3") {
        partial_interaction_m <- 0
        se_partial_m <- 0
      } else {
        partial_interaction_m <- coefficients[(3*m+7)]
        se_partial_m <- st_errors[(3*m+7)]
      } 
      if (tierdiff != "tier_diff_3" & wave != "Wave 1") {
        full_interaction_wm <- coefficients[paste0(pred,":factor(wave)", w, ":", tierdiff)]
        se_full_wm <- st_errors[paste0(pred,":factor(wave)", w, ":", tierdiff)]
      } else {
        full_interaction_wm <- 0
        se_full_wm <- 0
      }
    # Calculate total standard error
    se_total <- sqrt(se_partial_w^2 + se_partial_m^2 + se_full_wm^2 + st_errors[2]^2)
    # Store coefficient information
    interaction <- partial_interaction_w + partial_interaction_m + full_interaction_wm
    
    # Store coefficient
    full_coef[i] = save_coefficient(interaction)
    
    # Print interpretation and significance
    significant[i] <- is_significant(as.numeric(full_coef[i]), as.numeric((se_total)))
    print_interpretation(full_coef[i], wave, region="whole US", tier_descr[m], significant[i], pred, county, df)
    i <- i + 1
    }}}
}
```

Significant differential effects among "equality level." Spread is greater among counties of similar socioeconomic leve. Strong results across every wave but especially in wave 3. Effects are not as discernible among Urban counties. Both correlation length and local correlation are effective metrics for predicting covid to discern socioeconimic effects.




# Mask + Wave Models

## Prepare Model Combinations

```{r}
# Data sets: (1) all counties & (2) urban counties
datasets = list(all_mask_weekly_spatial_metrics)
df_names = c("all")

# Predictor: (1) correlation length & (2) local correlation
predictor = c("cor_lengths", "r_0_50") 
predictor_names = c("corlengths","localcor")

# Covariates (1) mhhinc & (2) mhhinc + wave
covariates = list(c("*factor(wave)*mask_11","*factor(wave)*mask_22","*factor(wave)*mask_33","*factor(wave)*mask_44"),
                  c("*mask_11","*mask_22","*mask_33", "*mask_44"))
var_names = c("wave_mask","mask")
```

## Generate Models

```{r, warning = FALSE}
# Create an empty list to store the models
mask_lm_models <- list()

# Iterate of every combination of data, predictor, and covariates
for (d in 1:length(datasets)) {
  df = datasets[d][[1]]
  for (p in 1:length(predictor)) {
    for (c in 1:length(covariates)) {
      interactions = c()
      for (i in 1:length(covariates[[c]])) {
        interactions[i] <- paste(predictor[p], covariates[[c]][i])}
     
      # Create the formula
      lm_formula <- as.formula(paste("log(next_week_marginal_cases) ~", paste(unlist(interactions), collapse = " + ")))
      
      # Fit the linear model
      lm_model <- lm(lm_formula, data = df)
      
      # Generate a name for the model object
      model_name <- paste(df_names[d],"_", predictor_names[p], "_", var_names[c], sep = "")
      
      # Save the model to the list
     mask_lm_models[[model_name]] <- lm_model
    }}}
```


### Training/Testing Models

```{r}
# Set seed for reproducibility
set.seed(1)

# Create an empty list to store the models
mask_lm_models_train <- list()

# Iterate over every combination of data, predictor, and covariates
for (d in 1:length(datasets)) {
  df <- datasets[[d]]
  
  # Create indices for train-test split
  n_obs <- nrow(df)
  train_indices <- sample(1:n_obs, size = round(train_ratio * n_obs), replace = FALSE)
  test_indices <- setdiff(1:n_obs, train_indices)
  
  # Split the data into train and test sets
  train_df <- df[train_indices, ]
  test_df <- df[test_indices, ]
  
  for (p in 1:length(predictor)) {
    for (c in 1:length(covariates)) {
      interactions <- c()
      for (i in 1:length(covariates[[c]])) {
        interactions[i] <- paste(predictor[p], covariates[[c]][i])
      }
      
      # Create the formula & fit linear model on the training data
      lm_formula <- as.formula(paste("log(next_week_marginal_cases) ~", paste(unlist(interactions), collapse = " + ")))
      lm_model_train <- lm(lm_formula, data = train_df)
      
      # Name model and save to list
      model_name_train <- paste(df_names[d], "_", predictor_names[p], "_", var_names[c], "_train", sep = "")
      mask_lm_models_train[[model_name_train]] <- lm_model_train
      
      # Apply the trained model to the test data and make predictions
      test_predictions <- predict(lm_model_train, newdata = test_df)
      
      # Calculate residuals using predictions from the test dataset
      residuals <- log(test_df$next_week_marginal_cases) - test_predictions
      
      # Create residual plot
      plot(1:length(residuals), residuals,
           main = paste("Residual Plot for", model_name_train),
           xlab = "Actual Values",
           ylab = "Residuals")
      abline(h = 0, col = "blue")
      
      # Check if the mean of residuals is less than the standard deviation of the data
      print(mean(abs(residuals), na.rm = TRUE) < sd(log(test_df$next_week_marginal_cases), na.rm = TRUE))
    }
  }
}

# Print the names of the models
names(mask_lm_models_train)

```


## View Model Summaries

```{r}
for (m in 1:length(mask_lm_models_train)){
  print(summary(mask_lm_models_train[[m]]))
}
names(mask_lm_models_train)
```


```{r, warning = FALSE}
# stargazer table for all_corlengths_wave_mask_train and all_corlengths_mask_train
# (1) is all
# (2) is urban 
stargazer(
  c(mask_lm_models_train[1], mask_lm_models_train[2]),
  type = 'text', 
  title = "Regression of Covid Cases on Correlation Length and Mask Usage"
)
```

```{r, warning = FALSE}
# stargazer table for all_localcor_wave_mask_train and all_localcor_mask_train
# (1) is all
# (2) is urban
stargazer(
  c(mhhinc_lm_models_train[3], mhhinc_lm_models_train[4]),
  type = 'text', 
  title = "Regression of Covid Cases on Local Correlation and Mask Usage"
)
```

## Check conditions of Normality of Residuals, Homoskedasticity,Linearity , No Perfect Multicollinearity, Independence of Residuals

```{r}
for (lm_model in mask_lm_models_train) {
# 1. Linearity Check (Visual Inspection)
plot(residuals(lm_model) ~ fitted(lm_model))

# 2. Independence Check
# Durbin-Watson test
print(durbinWatsonTest(lm_model)[3])

# 3. Homoscedasticity Check
# Breusch-Pagan test for Heteroscedasticity
print(bptest(lm_model)$p.value)

# 4. Normality of Residuals Check (Visual Inspection)
hist(residuals(lm_model), main = "Histogram of Residuals")
}

# 5. No Perfect Multicollinearity Check
# Variance Inflation Factors (VIF)
vif(lm(log(next_week_marginal_cases) ~  cor_lengths + factor(wave) + factor(mask_usage) , data = all_mask_weekly_spatial_metrics))
# Conditions not passed:
## Heteroskedasticity in all_localcor_mask_train
```

### Interpretations

```{r}
# Initialize values
predictor <- c("cor_lengths","r_0_50")
waves <- c("wave 1","wave 2", "wave 3")
usage_same <- c("mask_11", "mask_22", "mask_33", "mask_44")
usage_descr <- c("is less than 40%", "is between 40% and 50%", "is between 50% and 60%", "is greater than 60%")
full_coef = c()
full_coef1 = c()
significant = c()
significant1 = c()
i=1
j=1

# Iterate over Wave and Tier diff dummies
for (model in 1:length(mask_lm_models_train)) {
  
  # Get model stats
  lm_model = mask_lm_models_train[[model]]
  coefficients <- coef(lm_model)
  coef_names <- names(coef(lm_model))
  # se <- summary(lm_model)$coefficients[, "Std. Error"] use robust SE's
  se <- sqrt(diag(vcovHC(lm_model)))
  se_named <- rep(NA, length(coef_names))
  se_named[names(se)] <- se
  st_errors <- se_named[coef_names]
  
  # Based on model, decide if urban or all county df
  if (model <=4) {
    county = "all counties"
    df = all_mask_weekly_spatial_metrics}
  else {
    county = "urban counties"
    df = urban_mask_weekly_spatial_metrics}
  
  # Based on model, choose cor_length or local_cor as predictor
  if (model %in% c(1,2)) {pred = predictor[1]}
  else {pred = predictor[2]}
    
  # Wave dummies not included for even models
  if (model %% 2 == 0) {
    wave = "every wave"
    for (m in 1:length(usage_same)) {
      usage_comb <- usage_same[m]
      interaction = coefficients[5+m]
      se_total = sqrt(sum((st_errors[5+m])^2 + (st_errors[2])^2))
      # Store coefficient
      full_coef1[j] = save_coefficient(interaction)
    
      # Print interpretation and significance
      significant1[j] <- is_significant(as.numeric(full_coef1[j]), as.numeric((se_total)))
      print_interpretation(full_coef1[j], wave, region="whole US", tier_descr = "are disregarded", significant1[j], pred, county, df, usage_descr[m])}}
  
  # Wave dummies with mhhinc tier dummies
  else {
  # Iterate over each wave    
  for (w in 1:length(waves)) {
    wave <- waves[w]
    # Iterate of each tier difference
    for (m in 1:length(usage_same)) {
      usage_comb <- usage_same[m]
      if (wave == "Wave 1") {
        partial_interaction_w <- 0
        se_partial_w <- 0
      } else {
        partial_interaction_w <- coefficients[(7 + w)]
        se_partial_w <- st_errors[(7 + w)]
      }
 
        partial_interaction_m <- coefficients[(3*m+8)]
        se_partial_m <- st_errors[(3*m+8)]

      if (wave != "Wave 1") {
        full_interaction_wm <- coefficients[paste0(pred,":factor(wave)", w, ":", usage_comb)]
        se_full_wm <- st_errors[paste0(pred,":factor(wave)", w, ":", usage_comb)]
      } else {
        full_interaction_wm <- 0
        se_full_wm <- 0
      }
    # Calculate total standard error
    se_total <- sqrt(se_partial_w^2 + se_partial_m^2 + se_full_wm^2 + st_errors[2]^2)
    # Store coefficient information
    interaction <- partial_interaction_w + partial_interaction_m + full_interaction_wm
    
    # Store coefficient
    full_coef[i] = save_coefficient(interaction)
    
    # Print interpretation and significance
    significant[i] <- is_significant(as.numeric(full_coef[i]), as.numeric((se_total)))
    print_interpretation(full_coef[i], wave, region="whole US", tier_descr = "are disregarded", significant[i], pred, county, df, usage_descr[m])
    i <- i + 1
    }}}
}
```






