---
title: "Spatial Correlation Data Cleaning"
output: html_document
date: '2024-02-13'
---

```{r}
#load packages
library(readxl)
library(stringr)
library(dplyr)
library(geosphere)
```

# Prepare county-level data set with population and coordinate points

```{r}
# Read in national county data 
counties = read.table("2023_Gaz_counties_national.txt", header = TRUE, sep = "\t")

# Clean fips code column
counties$GEOID = str_pad(counties$GEOID, width = 5, side = "left", pad = "0")

# Drop cols
cols_drop = c('ANSICODE','ALAND','AWATER','ALAND_SQMI','AWATER_SQMI')
counties = counties[, !names(counties) %in% cols_drop]

# Merge with population
population = read_excel("PopulationEstimates.xlsx", skip = 4, col_names = TRUE)
counties_pop = merge(counties, population[, c("FIPStxt", "CENSUS_2020_POP")], 
                     by.x = "GEOID", by.y = "FIPStxt", all.x = TRUE)

# Drop counties with pop < 10k
county_subset = filter(counties_pop, CENSUS_2020_POP >10000)
nrow(county_subset) # 2229

# Rename cols
names(county_subset) = c('fips','state','county','latitude','longitude','population')
head(county_subset)
```

# Calculate pairwise distances between counties

```{r}
# Create all possible combinations of counties (unique pairs)
county_combinations <- expand.grid(county_subset$fips, county_subset$fips)
colnames(county_combinations) <- c("fips1", "fips2")

# Remove rows where fips1 is equal to fips2 to avoid self-comparisons
county_combinations <- county_combinations[county_combinations$fips1 != county_combinations$fips2, ]

# Merge with the original data to get the coordinates for each county
merged_data <- merge(county_combinations, county_subset, by.x = "fips1", by.y = "fips", all.x = TRUE)
merged_data <- merge(merged_data, county_subset, by.x = "fips2", by.y = "fips", all.x = TRUE, suffixes = c("_1", "_2"))

# Calculate distances between coordinate points
merged_data$distance <- distVincentySphere(
  cbind(merged_data$longitude_1, merged_data$latitude_1),
  cbind(merged_data$longitude_2, merged_data$latitude_2)
)
```

```{r}
# Convert distance to kilometers
merged_data$distance_km <- merged_data$distance / 1000

# Filter data
our_counties = filter(merged_data, distance_km <= 1000)

# Inspect
nrow(our_counties) # 1837374
head(our_counties)
```

```{r}
# Create r intervals: [r, r+rd]
r_upper = 50 * 1*(1:20)
r_lower = c(0,r_upper[1:19])

# Add r_i indexes 1,2,....
our_counties$r_i <- cut(
  our_counties$distance_km, 
  breaks = c(-Inf, r_upper, Inf), 
  labels = 1:(length(r_upper) + 1),  # Adjusted length for labels
  include.lowest = TRUE
)

# Add r col
our_counties$r <- round(r_upper[our_counties$r_i],2)

# Inspect distribution of r_i
our_counties$r_i = as.numeric(our_counties$r_i)
hist(our_counties$r_i)
table(our_counties$r_i)
```
# Aggregate Covid Data (from Meichen's code)

```{r}
# read in covid data
covid2020_raw <- read.csv("us-counties-2020.csv")
covid2021_raw <- read.csv("us-counties-2021.csv")
covid2022_raw <- read.csv("us-counties-2022.csv")
```

```{r}
# "starting on February 1 2020, we aggregate the total number of newly infected cases 
# in a given county over the previous 7 days (including the given day) 
# and calculate the daily average during this time period."

# I originally made this a function bc i was gonna apply it to all 3 
# covid datsets, but then I realized I should combine all 3 datasets
# and then run the function once, so the time is continuous 

create_daily_avg_by_week <- function(df, start_date){
    # convert fips to string and add leading 0 if needed 
    # (should be 5 char long)
    df$fips = as.character(df$fips)
    df$fips = str_pad(df$fips, width = 5, side = "left", pad = "0")
    
    # drop rows that have NA's for fips code
    df <- df[!is.na(df$fips), ]
    
    # Convert the date column to a Date object
    df$date <- as.Date(df$date, format = "%Y-%m-%d")

    # drop any rows that are recorded before start_date
    # retain rows recorded on start date
    df <- df %>% filter(date >= start_date) 

    # Create a week column that increases with every 7 days from the start date
    df$week <- as.integer(ceiling(as.numeric(df$date - start_date + 1) / 7))

    # Group by FIPS code and week, then summarize cases
    daily_avg <- df %>% group_by(fips, week) %>%
        summarize(total_cases = sum(cases), 
            avg_daily_cases = sum(cases) / 7,
            total_deaths = sum(deaths),
            avg_daily_deaths = sum(deaths) / 7, 
            .groups = 'drop')
    
    daily_avg
}

# combine all into one dataset 
covid_raw = rbind(covid2020_raw, covid2021_raw, covid2022_raw)
start_date <- as.Date("2020-01-26")
covid_daily_avg <- create_daily_avg_by_week(covid_raw, start_date)

# Add start date of week column
covid_daily_avg$start_date <- as.Date("2020-02-01") + (covid_daily_avg$week - 1) * 7

head(covid_daily_avg)
tail(covid_daily_avg)
```
```{r}
population_raw <- read_xlsx("PopulationEstimates.xlsx")
population <- population_raw[-(1:3), ]

# Set the column names of to be the values of the first row
colnames(population) <- as.character(unlist(population[1, ]))
population <- population[-1, ]
# remove first row of data since it's for entirety of USA
population <- population[-1, ]

# extract population estimates
population <- population[, c("FIPStxt", "POP_ESTIMATE_2020", "POP_ESTIMATE_2021", "POP_ESTIMATE_2022")]
population[, 2:4] <- lapply(population[, 2:4], as.numeric)

# add average population 2020 - 2022 as we define socioeconomic factors as fixed 
population$avg_pop <- rowMeans(population[, 2:4], na.rm = TRUE)

head(population)
```

```{r}
# "We then convert this number to the average daily fraction of the population 
# in each county that was infected during this week 
# by dividing with the county population." 

covid_daily_avg_pop <- covid_daily_avg %>%
    left_join(population, by = c("fips" = "FIPStxt")) %>%
    select(fips, week, total_cases, total_deaths, 
           avg_daily_cases, avg_daily_deaths, avg_pop)

head(covid_daily_avg_pop)
```

```{r}
# counties with no population data 
fips_no_pop_data <- covid_daily_avg_pop %>%
  filter(is.na(avg_pop)) %>%
  pull(fips) %>%
  unique()
print(fips_no_pop_data)
```

```{r}
# "We remove 697 counties with a population less than 10,000 
# because a small change in the number of cases in a small population 
# can lead to large fluctuations, which results in a total of 2411 counties"

covid_fract_daily_avg <- covid_daily_avg_pop %>%
  # Remove rows where avg_pop is less than 10000
  # or avg_pop is null
  filter(avg_pop >= 10000, !is.na(avg_pop)) %>%
  # divide by population
  mutate(fract_avg_daily_cases = avg_daily_cases / avg_pop,
         fract_avg_daily_deaths = avg_daily_deaths / avg_pop) %>%
  select(fips, week, total_cases, total_deaths, 
           fract_avg_daily_cases, fract_avg_daily_deaths, avg_pop)

head(covid_fract_daily_avg)
```

```{r}
# how many counties are there?
length(unique(covid_fract_daily_avg$fips))
# doesn't match with 2411 counties from paper 
```

# Detrend data: Add X_T(i) column

```{r}
# Sort the data frame by fips and week
covid_fract_daily_avg <- covid_fract_daily_avg[order(covid_fract_daily_avg$fips, covid_fract_daily_avg$week), ]

# Creat X_T column
covid_fract_daily_avg <- covid_fract_daily_avg %>%
  group_by(fips) %>%
  mutate(X_T = fract_avg_daily_cases - lag(fract_avg_daily_cases, default = 0))

head(covid_fract_daily_avg)
```

# Merge X_T onto distance data for fixed T

```{r}
# set T to week 100
covid_100 = filter(covid_fract_daily_avg, week ==100)
head(covid_100)
```

```{r}
# Merge data frames based on fips1
counties_covid_1 <- merge(our_counties, covid_100[, c("fips", "X_T")], by.x = "fips1", by.y = "fips", all.x = TRUE, suffixes = c("", "_1"))
names(counties_covid_1)[names(counties_covid_1) == "X_T"] <- "X_T_1"

# Merge data frames based on fips2
covid_counties_1_2 <- merge(counties_covid_1, covid_100[, c("fips", "X_T")], by.x = "fips2", by.y = "fips", all.x = TRUE, suffixes = c("", "_2"))
names(covid_counties_1_2)[names(covid_counties_1_2) == "X_T"] <- "X_T_2"

# Inspect
head(covid_counties_1_2)
```

```{r}
# Initialize empty vectors to store results
n = length(r_upper)
m_1 <- numeric(n)
m_2 <- numeric(n)
s2_1 <- numeric(n)
s2_2 <- numeric(n)
C_r_100 <- numeric(n)

# For loop to calculate means and variances for each unique r_i
for (i in 1:n) {
  subset_data_1 <- covid_counties_1_2$X_T_1[covid_counties_1_2$r_i == i]
  subset_data_2 <- covid_counties_1_2$X_T_2[covid_counties_1_2$r_i == i]
  
  # Calculate means
  m_1[i] <- mean(subset_data_1, na.rm = TRUE)
  m_2[i] <- mean(subset_data_2, na.rm = TRUE)
  
  # Calculate variances
  s2_1[i] <- sum((subset_data_1 - m_1[i])^2, na.rm = TRUE) / sum(!is.na(subset_data_1))
  s2_2[i] <- sum((subset_data_2 - m_2[i])^2, na.rm = TRUE) / sum(!is.na(subset_data_2))

}

# Correlation function
for (i in 1:n) {
  subset_data_1 <- covid_counties_1_2$X_T_1[covid_counties_1_2$r_i == i]
  subset_data_2 <- covid_counties_1_2$X_T_2[covid_counties_1_2$r_i == i]
  numerator = sum(subset_data_1*subset_data_2 - m_1[i]*m_2[i], na.rm = TRUE)/length(covid_counties_1_2$r_i == i)
  denominator = sqrt(s2_1[i]*s2_2[i])
  C_r_100[i]= numerator/denominator
}

```


```{r}
# Assuming you have vectors or data frames named 'C_r' and 'r_lower'
plot(r_lower, C_r_100, type = "l", xlab = "distance in km (r)", ylab = "C(r)", main = "Week 100")
```
```{r}
# Takes ~ 20 mins

list_Cr_all = list()

for (w in 1:150) {
  covid_week = filter(covid_fract_daily_avg, week == w)
  
  # Merge data frames based on fips1
  covid_counties_1 <- merge(our_counties, covid_week[, c("fips", "X_T")], by.x = "fips1", by.y = "fips", all.x = TRUE, suffixes = c("", "_1"))
  names(covid_counties_1)[names(covid_counties_1) == "X_T"] <- "X_T_1"
  # Merge data frames based on fips2
  covid_counties_1_2 <- merge(covid_counties_1, covid_week[, c("fips", "X_T")], by.x = "fips2", by.y = "fips", all.x = TRUE, suffixes = c("", "_2"))
  names(covid_counties_1_2)[names(covid_counties_1_2) == "X_T"] <- "X_T_2"
  # Initialize empty vectors to store results
  m_1 <- numeric(n)
  m_2 <- numeric(n)
  s2_1 <- numeric(n)
  s2_2 <- numeric(n)
  C_r <- numeric(n)

  # For loop to calculate means and variances for each unique r_i
  for (i in 1:n) {
    subset_data_1 <- covid_counties_1_2$X_T_1[covid_counties_1_2$r_i == i]
    subset_data_2 <- covid_counties_1_2$X_T_2[covid_counties_1_2$r_i == i]
  
    # Calculate means
    m_1[i] <- mean(subset_data_1, na.rm = TRUE)
    m_2[i] <- mean(subset_data_2, na.rm = TRUE)
  
    # Calculate variances
    s2_1[i] <- sum((subset_data_1 - m_1[i])^2, na.rm = TRUE) / sum(!is.na(subset_data_1))
    s2_2[i] <- sum((subset_data_2 - m_2[i])^2, na.rm = TRUE) / sum(!is.na(subset_data_2))

  }

  # Correlation function
  for (i in 1:n) {
    subset_data_1 <- covid_counties_1_2$X_T_1[covid_counties_1_2$r_i == i]
    subset_data_2 <- covid_counties_1_2$X_T_2[covid_counties_1_2$r_i == i]
    numerator = sum(subset_data_1*subset_data_2 - m_1[i]*m_2[i], na.rm = TRUE)/length(covid_counties_1_2$r_i == i)
    denominator = sqrt(s2_1[i]*s2_2[i])
    C_r[i]= numerator/denominator
  }
  list_Cr_all[[w]] <- C_r
}
```

```{r}
# Split list into 10 subsets
list_of_subsets <- split(list_Cr_all, rep(1:10, each = 15))

# Create a list to store matrix_Cr for each subset
list_of_matrix_Cr <- list()

# Iterate over each subset
for (i in seq_along(list_of_subsets)) {
  subset_list <- list_of_subsets[[i]]
  
  # Convert each subset into a matrix
  matrix_Cr <- sapply(subset_list, unlist)
  
  # Store the matrix in the list
  list_of_matrix_Cr[[i]] <- matrix_Cr
}

# Create 10 plots
par(mfrow = c(2, 5))  # Adjust the layout according to your preference

for (i in seq_along(list_of_matrix_Cr)) {
  # Plot using matplot
  matplot(r_lower, list_of_matrix_Cr[[i]], type = "l", col = 1:15, lty = 1:15,
          xlab = "r_lower", ylab = "C_r", main = paste("Weeks", (i - 1) * 15 + 1, "-", i * 15),
          legend.text = seq_along(list_of_matrix_Cr[[i]]), col.axis = "blue")
}
```


```{r}
# Inspect Weeks 10-20
matrix_Cr <- sapply(list_Cr_all[10:20], unlist)
matplot(r_lower, matrix_Cr, type = "l", col = 1:10, lty = 1:10,
        xlab = "r_lower", ylab = "C_r", main = "Weeks 10-20",
        legend.text = seq_along(list_Cr), col.axis = "blue")
```



